{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "20d78c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "import openpyxl #leer y escribir en excel \n",
    "import sqlalchemy, pymysql #conectar python con la base de datos Mysql"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f893735",
   "metadata": {},
   "source": [
    "# ETL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "89791993",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os \n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "477b2bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "RUTA_TR = \"./DCE_DATOS_2025/TR\"\n",
    "\n",
    "MAPA_SENSORES = {\n",
    "    \"01_VOLTAJE_AC_DEL_SISTEMA_L1_L2\": \"VOLTAJE_AC_L1_l2\",\n",
    "    \"02_VOLTAJE_AC_DEL_SISTEMA_L2-L3\": \"VOLTAJE_AC_L2_l3\",\n",
    "    \"03_VOLTAJE_AC_DEL_SISTEMA_L3-L1\": \"VOLTAJE_AC_L3_l1\",\n",
    "    \"11_CORRIENTE_AC_DE_LA_CARGA_L1\": \"CORRIENTE_AC_L1\",\n",
    "    \"12_CORRIENTE_AC_DE_LA_CARGA_L2\": \"CORRIENTE_AC_L2\",\n",
    "    \"13_CORRIENTE_AC_DE_LA_CARGA_L3\": \"CORRIENTE_AC_L3\",\n",
    "    \"14_POTENCIA_ACTIVA_DE_LA_CARGA\": \"POTENCIA_ACTIVA_KW\",\n",
    "    \"15_POTENCIA_REACTIVA_DE_LA_CARGA\": \"POTENCIA_REACTIVA_KVAR\",\n",
    "    \"16_POTENCIA_APARENTE_DE_LA_CARGA\": \"POTENCIA_APARENTE_KVA\",\n",
    "    \"17_FACTOR_DE_POTENCIA_DE_LA_CARGA\": \"FACTOR_DE_POTENCIA\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1340d8a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def limpiar_fecha(fecha_str):\n",
    "    \"\"\"\n",
    "    TR - IDEO CALI,\"01 - VOLTAJE AC DEL SISTEMA L1-L2\",\"216.0\",\"V\",\"1/1/2025, 12:50:03 a.Â m.\"\n",
    "\n",
    "    Arregla el formato y elimina caracteres de codificación corruptos (Â).\n",
    "    Entrada: '1/1/2025, 12:00:00 a.Â m.' o '1/1/2025, 12:00:00 a.\\xa0m.'\n",
    "    Salida: '1/1/2025 12:00:00 AM'\n",
    "    \"\"\"\n",
    "\n",
    "    if not isinstance(fecha_str, str):\n",
    "        return str(fecha_str)\n",
    "    \n",
    "\n",
    "    limpia = fecha_str.replace(\"Â\", \"\") \\\n",
    "                      .replace(\"\\xa0\", \" \") \\\n",
    "                      .replace(\",\", \"\") \n",
    "    \n",
    "    limpia = limpia.lower() \\\n",
    "                   .replace(\"a. m.\", \"AM\") \\\n",
    "                   .replace(\"p. m.\", \"PM\") \\\n",
    "                   .replace(\"a.m.\", \"AM\") \\\n",
    "                   .replace(\"p.m.\", \"PM\") \\\n",
    "                   .replace(\"am\", \"AM\") \\\n",
    "                   .replace(\"pm\", \"PM\")\n",
    "    \n",
    "    return limpia.strip() # Eliminar espacios "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d996daa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12:50:03 AM\n"
     ]
    }
   ],
   "source": [
    "fecha= \"12:50:03 a.Â m.\"\n",
    "fecha1 = limpiar_fecha(fecha)\n",
    "print(fecha1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b09239da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def procesar_carpeta_sensor(nombre_carpeta, nombre_columna_db):\n",
    "    \"\"\"\n",
    "    Lee los 12 meses de una carpeta, limpia y consolida.\n",
    "    \"\"\"\n",
    "\n",
    "    ruta_busqueda = os.path.join(RUTA_TR, nombre_carpeta, \"*.csv\") # O *.cvs si tienen error de extension\n",
    "    archivos = glob.glob(ruta_busqueda)\n",
    "    \n",
    "    if not archivos:\n",
    "        print(f\"ADVERTENCIA: No se encontraron archivos en {nombre_carpeta}\")\n",
    "        return None\n",
    "\n",
    "    print(f\"Procesando {nombre_carpeta} ({len(archivos)} archivos)...\")\n",
    "    \n",
    "    dfs_meses = []\n",
    "    \n",
    "    for archivo in archivos:\n",
    "        try:\n",
    "            # Leemos el CSV. Importante: dtype=str para no romper la lectura con 'Unplugged'\n",
    "            df_temp = pd.read_csv(archivo, encoding='latin-1', dtype=str) # Probamos latin-1 por los caracteres raros\n",
    "            \n",
    "            df_temp['Time_Clean'] = df_temp['Time'].apply(limpiar_fecha)\n",
    "            df_temp['timestamp'] = pd.to_datetime(df_temp['Time_Clean'], format='%d/%m/%Y %I:%M:%S %p', errors='coerce')\n",
    "            \n",
    "            # Convertimos Value a números. Los textos se vuelven NaN.\n",
    "            df_temp['valor_numerico'] = pd.to_numeric(df_temp['Value'], errors='coerce')\n",
    "            \n",
    "            #Eliminar filas con NaN\n",
    "            df_temp = df_temp.dropna(subset=['valor_numerico', 'timestamp'])\n",
    "            \n",
    "            # Seleccionamos solo lo que sirve (ya limpio)\n",
    "            df_util = df_temp[['timestamp', 'valor_numerico']].copy()\n",
    "            \n",
    "            dfs_meses.append(df_util)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error leyendo {os.path.basename(archivo)}: {e}\")\n",
    "\n",
    "    if not dfs_meses:\n",
    "        return None\n",
    "\n",
    "    # Unimos Enero-Diciembre verticalmente\n",
    "    df_anual = pd.concat(dfs_meses, ignore_index=True)\n",
    "    \n",
    "    # Ordenamos por fecha\n",
    "    df_anual = df_anual.sort_values('timestamp')\n",
    "    df_anual = df_anual.set_index('timestamp')\n",
    "    \n",
    "    # 3. RESAMPLING (Normalización de tiempo)\n",
    "    # Como los datos vienen a 12:01:59 y otros a 12:05:00, promediamos cada 10 minutos\n",
    "    # Esto alinea los datos y reduce el ruido.\n",
    "    df_resampled = df_anual.resample('10min').mean()\n",
    "    \n",
    "    # Renombramos la columna 'valor_numerico' al nombre real del sensor (ej: voltaje_ac_l1_l2)\n",
    "    df_resampled.columns = [nombre_columna_db]\n",
    "    \n",
    "    return df_resampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6a0f445c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ETL para Elemento TR...\n",
      "Procesando 01_VOLTAJE_AC_DEL_SISTEMA_L1_L2 (12 archivos)...\n",
      "Procesando 02_VOLTAJE_AC_DEL_SISTEMA_L2-L3 (12 archivos)...\n",
      "Procesando 03_VOLTAJE_AC_DEL_SISTEMA_L3-L1 (12 archivos)...\n",
      "Procesando 11_CORRIENTE_AC_DE_LA_CARGA_L1 (12 archivos)...\n",
      "Procesando 12_CORRIENTE_AC_DE_LA_CARGA_L2 (12 archivos)...\n",
      "Procesando 13_CORRIENTE_AC_DE_LA_CARGA_L3 (12 archivos)...\n",
      "Procesando 14_POTENCIA_ACTIVA_DE_LA_CARGA (12 archivos)...\n",
      "Procesando 15_POTENCIA_REACTIVA_DE_LA_CARGA (12 archivos)...\n",
      "Procesando 16_POTENCIA_APARENTE_DE_LA_CARGA (12 archivos)...\n",
      "Procesando 17_FACTOR_DE_POTENCIA_DE_LA_CARGA (12 archivos)...\n",
      "\n",
      " Unificando todos los sensores en una sola tabla maestra...\n",
      " Tabla consolidada creada: 52560 registros x 11 columnas\n",
      "            timestamp  VOLTAJE_AC_L1_l2  VOLTAJE_AC_L2_l3  VOLTAJE_AC_L3_l1  \\\n",
      "0 2025-01-01 00:00:00        215.250000        215.000000        216.750000   \n",
      "1 2025-01-01 00:10:00        215.333333        215.333333        216.333333   \n",
      "2 2025-01-01 00:20:00        215.750000        215.250000        217.000000   \n",
      "3 2025-01-01 00:30:00        215.666667        215.000000        216.666667   \n",
      "4 2025-01-01 00:40:00        216.000000        215.333333        217.000000   \n",
      "\n",
      "   CORRIENTE_AC_L1  CORRIENTE_AC_L2  CORRIENTE_AC_L3  POTENCIA_ACTIVA_KW  \\\n",
      "0        58.000000        58.000000        59.000000           21.000000   \n",
      "1        63.000000        63.000000        63.333333           23.333333   \n",
      "2        60.250000        60.250000        61.000000           21.750000   \n",
      "3        60.333333        60.333333        61.333333           22.000000   \n",
      "4        60.666667        60.666667        61.333333           22.000000   \n",
      "\n",
      "   POTENCIA_REACTIVA_KVAR  POTENCIA_APARENTE_KVA  FACTOR_DE_POTENCIA  \n",
      "0               -5.000000              22.000000            0.970000  \n",
      "1               -3.333333              23.666667            0.983333  \n",
      "2               -4.500000              22.750000            0.975000  \n",
      "3               -4.333333              22.666667            0.976667  \n",
      "4               -4.333333              23.000000            0.976667  \n",
      "\n",
      " Archivo guardado: consolidado_TR_2025.csv\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    print(\"ETL para Elemento TR...\")\n",
    "    \n",
    "    dataframes_sensores = []\n",
    "    \n",
    "    # Recorremos cada carpeta definida en el mapa\n",
    "    for carpeta_original, columna_destino in MAPA_SENSORES.items():\n",
    "        df_sensor = procesar_carpeta_sensor(carpeta_original, columna_destino)\n",
    "        \n",
    "        if df_sensor is not None:\n",
    "            dataframes_sensores.append(df_sensor)\n",
    "    \n",
    "    if dataframes_sensores:\n",
    "        print(\"\\n Unificando todos los sensores en una sola tabla maestra...\")\n",
    "        \n",
    "        # Unimos todos los dataframes usando el índice (timestamp)\n",
    "        # axis=1 significa pegar las columnas una al lado de la otra\n",
    "        df_final_tr = pd.concat(dataframes_sensores, axis=1)\n",
    "        \n",
    "        # Opcional: Llenar huecos pequeños (si un sensor falló 10 min) o dejarlos como NaN\n",
    "        # df_final_tr = df_final_tr.interpolate(method='time') \n",
    "        \n",
    "        # Reset index para que timestamp sea una columna\n",
    "        df_final_tr = df_final_tr.reset_index()\n",
    "        \n",
    "        print(f\" Tabla consolidada creada: {df_final_tr.shape[0]} registros x {df_final_tr.shape[1]} columnas\")\n",
    "        print(df_final_tr.head())\n",
    "        \n",
    "        # GUARDAR \n",
    "        archivo_salida = \"consolidado_TR_2025.csv\" \n",
    "        df_final_tr.to_csv(archivo_salida, index=False)\n",
    "        print(f\"\\n Archivo guardado: {archivo_salida}\")\n",
    "        \n",
    "    else:\n",
    "        print(\"No se pudieron procesar datos.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
